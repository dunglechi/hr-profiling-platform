"""
DISC Assessment External Data Pipeline
H·ªó tr·ª£ import file CSV/PDF/Excel v√† OCR ·∫£nh survey
"""

import pandas as pd
import json
import base64
from typing import Dict, List, Any, Optional, Union
from datetime import datetime
from io import BytesIO
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DISCExternalPipeline:
    """
    Pipeline x·ª≠ l√Ω d·ªØ li·ªáu DISC t·ª´ c√°c ngu·ªìn b√™n ngo√†i
    """
    
    # Standard DISC question mapping
    DISC_STANDARD_QUESTIONS = {
        "D": [
            "T√¥i th√≠ch l√†m vi·ªác v·ªõi nh·ªØng th√°ch th·ª©c kh√≥ khƒÉn",
            "T√¥i th∆∞·ªùng ƒë∆∞a ra quy·∫øt ƒë·ªãnh nhanh ch√≥ng",
            "T√¥i th√≠ch ki·ªÉm so√°t t√¨nh h√¨nh",
            "T√¥i kh√¥ng ng·∫°i ƒë·ªëi ƒë·∫ßu khi c·∫ßn thi·∫øt",
            "T√¥i th∆∞·ªùng l√† ng∆∞·ªùi ƒëi ƒë·∫ßu trong nh√≥m"
        ],
        "I": [
            "T√¥i th√≠ch g·∫∑p g·ª° ng∆∞·ªùi m·ªõi",
            "T√¥i hay k·ªÉ chuy·ªán v√† chia s·∫ª",
            "T√¥i th√≠ch l√†m vi·ªác nh√≥m",
            "T√¥i l·∫°c quan v√† t√≠ch c·ª±c",
            "T√¥i th√≠ch ƒë∆∞·ª£c c√¥ng nh·∫≠n v√† khen ng·ª£i"
        ],
        "S": [
            "T√¥i th√≠ch s·ª± ·ªïn ƒë·ªãnh v√† d·ª± ƒëo√°n ƒë∆∞·ª£c",
            "T√¥i l√†m vi·ªác ch·∫≠m r√£i nh∆∞ng ch·∫Øc ch·∫Øn",
            "T√¥i th√≠ch gi√∫p ƒë·ª° ng∆∞·ªùi kh√°c",
            "T√¥i tr√°nh xung ƒë·ªôt",
            "T√¥i th√≠ch l√†m vi·ªác trong m√¥i tr∆∞·ªùng th√¢n thi·ªán"
        ],
        "C": [
            "T√¥i ch√∫ tr·ªçng ƒë·∫øn chi ti·∫øt",
            "T√¥i th√≠ch l√†m vi·ªác theo quy tr√¨nh",
            "T√¥i c·∫ßn th√¥ng tin ƒë·∫ßy ƒë·ªß tr∆∞·ªõc khi quy·∫øt ƒë·ªãnh",
            "T√¥i th√≠ch s·ª± ch√≠nh x√°c",
            "T√¥i th∆∞·ªùng ph√¢n t√≠ch k·ªπ tr∆∞·ªõc khi h√†nh ƒë·ªông"
        ]
    }
    
    def __init__(self):
        self.processing_log = []
        
    def validate_disc_scores(self, scores: Dict[str, float]) -> Dict[str, Any]:
        """
        Validate DISC scores format and values
        """
        validation_result = {
            "valid": True,
            "errors": [],
            "warnings": [],
            "normalized_scores": {}
        }
        
        required_dimensions = ["D", "I", "S", "C"]
        
        # Check required dimensions
        for dim in required_dimensions:
            if dim not in scores:
                validation_result["errors"].append(f"Missing DISC dimension: {dim}")
                validation_result["valid"] = False
            else:
                score = scores[dim]
                
                # Validate score range
                if not isinstance(score, (int, float)):
                    validation_result["errors"].append(f"Invalid score type for {dim}: {type(score)}")
                    validation_result["valid"] = False
                elif score < 0 or score > 100:
                    validation_result["errors"].append(f"Score out of range for {dim}: {score} (must be 0-100)")
                    validation_result["valid"] = False
                else:
                    validation_result["normalized_scores"][dim] = float(score)
        
        # Check total score distribution
        if validation_result["valid"]:
            total = sum(validation_result["normalized_scores"].values())
            if abs(total - 100) > 10:  # Allow 10% tolerance
                validation_result["warnings"].append(f"Total scores deviate from 100%: {total}%")
        
        return validation_result
    
    def process_csv_upload(self, csv_content: str, candidate_id: str) -> Dict[str, Any]:
        """
        Process CSV file upload v·ªõi DISC scores
        Expected format: candidate_id,D,I,S,C,notes
        """
        try:
            # Parse CSV
            df = pd.read_csv(BytesIO(csv_content.encode()))
            
            result = {
                "success": False,
                "source": "csv",
                "candidate_id": candidate_id,
                "timestamp": datetime.now().isoformat(),
                "data": None,
                "errors": [],
                "warnings": []
            }
            
            # Validate CSV structure
            required_columns = ["candidate_id", "D", "I", "S", "C"]
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                result["errors"].append(f"Missing required columns: {missing_columns}")
                return result
            
            # Find candidate row
            candidate_row = df[df["candidate_id"] == candidate_id]
            
            if candidate_row.empty:
                result["errors"].append(f"Candidate ID {candidate_id} not found in CSV")
                return result
            
            # Extract DISC scores
            row = candidate_row.iloc[0]
            disc_scores = {
                "D": row["D"],
                "I": row["I"], 
                "S": row["S"],
                "C": row["C"]
            }
            
            # Validate scores
            validation = self.validate_disc_scores(disc_scores)
            
            if not validation["valid"]:
                result["errors"].extend(validation["errors"])
                return result
            
            # Calculate derived metrics
            derived_metrics = self.calculate_disc_metrics(validation["normalized_scores"])
            
            result.update({
                "success": True,
                "data": {
                    "raw_scores": validation["normalized_scores"],
                    "primary_style": derived_metrics["primary_style"],
                    "secondary_style": derived_metrics["secondary_style"],
                    "style_intensity": derived_metrics["style_intensity"],
                    "behavioral_description": derived_metrics["description"],
                    "notes": row.get("notes", ""),
                    "upload_method": "csv_file"
                },
                "warnings": validation["warnings"]
            })
            
            logger.info(f"CSV DISC data processed successfully for candidate {candidate_id}")
            return result
            
        except Exception as e:
            logger.error(f"CSV processing error for candidate {candidate_id}: {str(e)}")
            return {
                "success": False,
                "source": "csv",
                "candidate_id": candidate_id,
                "timestamp": datetime.now().isoformat(),
                "errors": [f"CSV processing error: {str(e)}"]
            }
    
    def process_manual_input(self, manual_data: Dict[str, Any], candidate_id: str) -> Dict[str, Any]:
        """
        Process manual DISC input t·ª´ recruiter
        """
        try:
            result = {
                "success": False,
                "source": "manual_input",
                "candidate_id": candidate_id,
                "timestamp": datetime.now().isoformat(),
                "data": None,
                "errors": [],
                "warnings": []
            }
            
            # Extract scores from manual input
            disc_scores = {
                "D": manual_data.get("d_score"),
                "I": manual_data.get("i_score"),
                "S": manual_data.get("s_score"),
                "C": manual_data.get("c_score")
            }
            
            # Validate scores
            validation = self.validate_disc_scores(disc_scores)
            
            if not validation["valid"]:
                result["errors"].extend(validation["errors"])
                return result
            
            # Calculate derived metrics
            derived_metrics = self.calculate_disc_metrics(validation["normalized_scores"])
            
            result.update({
                "success": True,
                "data": {
                    "raw_scores": validation["normalized_scores"],
                    "primary_style": derived_metrics["primary_style"],
                    "secondary_style": derived_metrics["secondary_style"],
                    "style_intensity": derived_metrics["style_intensity"],
                    "behavioral_description": derived_metrics["description"],
                    "notes": manual_data.get("notes", ""),
                    "recruiter_id": manual_data.get("recruiter_id"),
                    "upload_method": "manual_input"
                },
                "warnings": validation["warnings"]
            })
            
            logger.info(f"Manual DISC data processed successfully for candidate {candidate_id}")
            return result
            
        except Exception as e:
            logger.error(f"Manual input processing error for candidate {candidate_id}: {str(e)}")
            return {
                "success": False,
                "source": "manual_input",
                "candidate_id": candidate_id,
                "timestamp": datetime.now().isoformat(),
                "errors": [f"Manual input processing error: {str(e)}"]
            }
    
    def generate_printable_survey(self) -> Dict[str, Any]:
        """
        T·∫°o b·∫£ng kh·∫£o s√°t DISC c√≥ th·ªÉ in ƒë·ªÉ OCR
        """
        try:
            survey_template = {
                "title": "B·∫¢NG KH·∫¢O S√ÅT ƒê√ÅNH GI√Å DISC",
                "instructions": [
                    "1. ƒê·ªçc k·ªπ t·ª´ng c√¢u h·ªèi d∆∞·ªõi ƒë√¢y",
                    "2. ƒê√°nh gi√° m·ª©c ƒë·ªô ph√π h·ª£p v·ªõi b·∫£n th√¢n t·ª´ 1-5",
                    "3. ƒêi·ªÅn s·ªë v√†o √¥ vu√¥ng b√™n c·∫°nh m·ªói c√¢u",
                    "4. Ch·ª•p ·∫£nh r√µ n√©t ƒë·ªÉ upload v√†o h·ªá th·ªëng"
                ],
                "scale": {
                    "1": "Ho√†n to√†n kh√¥ng ph√π h·ª£p",
                    "2": "√çt ph√π h·ª£p", 
                    "3": "Trung b√¨nh",
                    "4": "Kh√° ph√π h·ª£p",
                    "5": "Ho√†n to√†n ph√π h·ª£p"
                },
                "questions": []
            }
            
            question_id = 1
            for dimension, questions in self.DISC_STANDARD_QUESTIONS.items():
                for question in questions:
                    survey_template["questions"].append({
                        "id": f"Q{question_id:02d}",
                        "dimension": dimension,
                        "text": question,
                        "score_box": "‚ñ°"  # OCR detection box
                    })
                    question_id += 1
            
            # Add candidate info section
            survey_template["candidate_info"] = {
                "name": "H·ªç t√™n: ________________________",
                "date": "Ng√†y l√†m b√†i: _______________",
                "signature": "Ch·ªØ k√Ω: ____________________"
            }
            
            # OCR processing instructions
            survey_template["ocr_instructions"] = [
                "üì∏ H∆Ø·ªöNG D·∫™N CH·ª§P ·∫¢NH CHO OCR:",
                "‚Ä¢ ƒê·∫∑t gi·∫•y tr√™n n·ªÅn s√°ng, ph·∫≥ng",
                "‚Ä¢ Ch·ª•p vu√¥ng g√≥c, kh√¥ng b·ªã nghi√™ng",
                "‚Ä¢ ƒê·∫£m b·∫£o √°nh s√°ng ƒë·ªÅu, kh√¥ng c√≥ b√≥ng",
                "‚Ä¢ Ch·ªØ s·ªë trong √¥ vu√¥ng ph·∫£i r√µ n√©t",
                "‚Ä¢ Upload file ·∫£nh ƒë·ªãnh d·∫°ng JPG/PNG"
            ]
            
            return {
                "success": True,
                "template": survey_template,
                "total_questions": len(survey_template["questions"]),
                "estimated_time": "10-15 ph√∫t",
                "ocr_ready": True
            }
            
        except Exception as e:
            logger.error(f"Survey generation error: {str(e)}")
            return {
                "success": False,
                "error": f"L·ªói t·∫°o b·∫£ng kh·∫£o s√°t: {str(e)}"
            }
    
    def process_ocr_image(self, image_data: str, candidate_id: str) -> Dict[str, Any]:
        """
        Process OCR image upload (mock implementation)
        Trong th·ª±c t·∫ø s·∫Ω t√≠ch h·ª£p v·ªõi OCR service nh∆∞ Tesseract, Google Vision API
        """
        try:
            # Mock OCR processing result
            # Trong th·ª±c t·∫ø s·∫Ω g·ªçi OCR service ƒë·ªÉ extract text/numbers
            mock_ocr_result = {
                "detected_answers": {
                    "Q01": 4, "Q02": 5, "Q03": 3, "Q04": 4, "Q05": 4,  # D questions
                    "Q06": 3, "Q07": 2, "Q08": 3, "Q09": 4, "Q10": 2,  # I questions  
                    "Q11": 4, "Q12": 5, "Q13": 4, "Q14": 5, "Q15": 4,  # S questions
                    "Q16": 5, "Q17": 4, "Q18": 5, "Q19": 4, "Q20": 5   # C questions
                },
                "confidence_scores": {
                    "overall": 0.89,
                    "low_confidence_questions": ["Q07", "Q10"]
                }
            }
            
            # Calculate DISC scores from OCR answers
            dimension_scores = {"D": 0, "I": 0, "S": 0, "C": 0}
            questions_per_dimension = 5
            
            for q_id, score in mock_ocr_result["detected_answers"].items():
                q_num = int(q_id[1:])
                if q_num <= 5:
                    dimension_scores["D"] += score
                elif q_num <= 10:
                    dimension_scores["I"] += score
                elif q_num <= 15:
                    dimension_scores["S"] += score
                else:
                    dimension_scores["C"] += score
            
            # Normalize to percentage
            max_score_per_dimension = questions_per_dimension * 5
            normalized_scores = {
                dim: (score / max_score_per_dimension) * 100 
                for dim, score in dimension_scores.items()
            }
            
            # Validate OCR results
            validation = self.validate_disc_scores(normalized_scores)
            
            result = {
                "success": validation["valid"],
                "source": "ocr_image",
                "candidate_id": candidate_id,
                "timestamp": datetime.now().isoformat(),
                "ocr_metadata": {
                    "confidence": mock_ocr_result["confidence_scores"]["overall"],
                    "low_confidence_items": mock_ocr_result["confidence_scores"]["low_confidence_questions"],
                    "total_questions_detected": len(mock_ocr_result["detected_answers"])
                },
                "errors": validation.get("errors", []),
                "warnings": validation.get("warnings", [])
            }
            
            if validation["valid"]:
                derived_metrics = self.calculate_disc_metrics(validation["normalized_scores"])
                result["data"] = {
                    "raw_scores": validation["normalized_scores"],
                    "primary_style": derived_metrics["primary_style"],
                    "secondary_style": derived_metrics["secondary_style"],
                    "style_intensity": derived_metrics["style_intensity"],
                    "behavioral_description": derived_metrics["description"],
                    "upload_method": "ocr_image",
                    "requires_manual_review": mock_ocr_result["confidence_scores"]["overall"] < 0.8
                }
            
            if mock_ocr_result["confidence_scores"]["overall"] < 0.8:
                result["warnings"].append("ƒê·ªô tin c·∫≠y OCR th·∫•p - c·∫ßn review th·ªß c√¥ng")
            
            logger.info(f"OCR DISC processing completed for candidate {candidate_id} - Confidence: {mock_ocr_result['confidence_scores']['overall']}")
            
            return result
            
        except Exception as e:
            logger.error(f"OCR processing error for candidate {candidate_id}: {str(e)}")
            return {
                "success": False,
                "source": "ocr_image",
                "candidate_id": candidate_id,
                "timestamp": datetime.now().isoformat(),
                "errors": [f"OCR processing error: {str(e)}"]
            }
    
    def calculate_disc_metrics(self, scores: Dict[str, float]) -> Dict[str, Any]:
        """
        Calculate derived DISC metrics from raw scores
        """
        # Find primary and secondary styles
        sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        primary_style = sorted_scores[0][0]
        primary_score = sorted_scores[0][1]
        secondary_style = sorted_scores[1][0]
        secondary_score = sorted_scores[1][1]
        
        # Calculate style intensity
        style_intensity = "high" if primary_score > 70 else "moderate" if primary_score > 50 else "low"
        
        # Generate behavioral description
        style_descriptions = {
            "D": "Quy·∫øt ƒëo√°n, th√≠ch th√°ch th·ª©c, h∆∞·ªõng ƒë·∫øn k·∫øt qu·∫£",
            "I": "T∆∞∆°ng t√°c, l·∫°c quan, th√≠ch giao ti·∫øp v√† l√†m vi·ªác nh√≥m",
            "S": "·ªîn ƒë·ªãnh, ki√™n nh·∫´n, h·ªó tr·ª£ v√† h·ª£p t√°c t·ªët",
            "C": "C·∫©n th·∫≠n, ch√≠nh x√°c, th√≠ch ph√¢n t√≠ch v√† tu√¢n th·ªß quy tr√¨nh"
        }
        
        description = f"Phong c√°ch ch√≠nh: {style_descriptions[primary_style]}. "
        if secondary_score > 30:
            description += f"Phong c√°ch ph·ª•: {style_descriptions[secondary_style]}."
        
        return {
            "primary_style": primary_style,
            "secondary_style": secondary_style,
            "style_intensity": style_intensity,
            "description": description,
            "score_distribution": scores
        }

# Test the pipeline
if __name__ == "__main__":
    pipeline = DISCExternalPipeline()
    
    # Test CSV processing
    csv_content = """candidate_id,D,I,S,C,notes
123,75,65,45,80,Candidate shows strong analytical skills
456,60,85,70,50,Very social and team-oriented"""
    
    print("=== CSV Processing Test ===")
    result = pipeline.process_csv_upload(csv_content, "123")
    print(f"Success: {result['success']}")
    if result['success']:
        print(f"Primary style: {result['data']['primary_style']}")
        print(f"Description: {result['data']['behavioral_description']}")
    
    # Test manual input
    print("\n=== Manual Input Test ===")
    manual_data = {
        "d_score": 70,
        "i_score": 80,
        "s_score": 60,
        "c_score": 75,
        "notes": "Manual assessment by recruiter",
        "recruiter_id": "recruiter_001"
    }
    
    result = pipeline.process_manual_input(manual_data, "456")
    print(f"Success: {result['success']}")
    if result['success']:
        print(f"Primary style: {result['data']['primary_style']}")
    
    # Test survey generation
    print("\n=== Survey Generation Test ===")
    survey = pipeline.generate_printable_survey()
    print(f"Survey generated: {survey['success']}")
    print(f"Total questions: {survey['template']['total_questions']}")
    
    # Test OCR processing
    print("\n=== OCR Processing Test ===")
    result = pipeline.process_ocr_image("mock_image_data", "789")
    print(f"OCR Success: {result['success']}")
    if result['success']:
        print(f"OCR Confidence: {result['ocr_metadata']['confidence']}")
        print(f"Primary style: {result['data']['primary_style']}")